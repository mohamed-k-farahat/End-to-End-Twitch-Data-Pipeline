# ğŸ® End-to-End Twitch Data Pipeline

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.0+-red.svg)
![dbt](https://img.shields.io/badge/dbt-1.0+-orange.svg)
![Snowflake](https://img.shields.io/badge/Snowflake-Data%20Warehouse-29B5E8.svg)
![Azure](https://img.shields.io/badge/Azure-Blob%20Storage-0078D4.svg)

*A production-ready data pipeline that extracts real-time streaming data from Twitch, orchestrates it through Azure and Snowflake, and transforms it into analytics-ready datasets.*

</div>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [Features](#-features)
- [Tech Stack](#-tech-stack)
- [Pipeline Flow](#-pipeline-flow)
- [Project Structure](#-project-structure)
- [Prerequisites](#-prerequisites)
- [Setup](#-setup)
- [Configuration](#-configuration)
- [Running the Pipeline](#-running-the-pipeline)
- [Data Models](#-data-models)
- [Monitoring](#-monitoring)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Overview

This project implements a **modern data engineering pipeline** that:

- ğŸ“¡ **Extracts** live streaming data from the Twitch API every 15 minutes
- ğŸ’¾ **Loads** raw JSON files into Azure Blob Storage for data lake storage
- ğŸ”„ **Ingests** data into Snowflake using Snowpipes for near real-time loading
- ğŸ”¨ **Transforms** raw data into dimensional models using dbt
- ğŸ“Š **Produces** analytics-ready fact and dimension tables

The pipeline is orchestrated using **Apache Airflow** on Astronomer, providing robust scheduling, monitoring, and error handling capabilities.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Twitch API    â”‚  â† Live streaming data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Apache Airflow  â”‚  â† Orchestration (Every 15 min)
â”‚  (Astronomer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Azure Blob      â”‚  â† Raw JSON storage (Data Lake)
â”‚   Storage       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Snowpipe      â”‚  â† Auto-ingest on file arrival
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Snowflake     â”‚  â† Data Warehouse
â”‚  (Raw Tables)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt Models    â”‚  â† Transformation (Hourly)
â”‚ (Staging/Marts) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Analytics     â”‚  â† BI Tools / Data Science
â”‚     Layer       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ¨ Features

### ğŸš€ Data Ingestion
- **Real-time extraction** from Twitch API (streams, users, games)
- **Batch processing** of 100+ concurrent streams
- **Automatic retries** and error handling
- **Timestamped snapshots** for historical tracking

### ğŸ”„ Data Transformation
- **Incremental models** for efficient processing
- **Dimensional modeling** (star schema)
- **Data quality tests** with dbt
- **Idempotent transformations** using merge strategies

### ğŸ“Š Analytics Ready
- **Fact tables** for stream snapshots with temporal tracking
- **Dimension tables** for streamers and games
- **Pre-aggregated metrics** for dashboard consumption
- **Type-2 slowly changing dimensions** support

### ğŸ› ï¸ DevOps & Monitoring
- **Containerized** deployment with Docker
- **Version controlled** infrastructure
- **Airflow UI** for pipeline monitoring
- **dbt docs** for data lineage and documentation

---

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | Apache Airflow (Astronomer) | Workflow scheduling and management |
| **Data Lake** | Azure Blob Storage | Raw data storage |
| **Data Warehouse** | Snowflake | Analytics database |
| **Transformation** | dbt (Data Build Tool) | SQL-based transformations |
| **Language** | Python 3.9+ | API extraction and scripting |
| **Infrastructure** | Docker | Containerization |
| **Version Control** | Git | Source code management |

### Key Python Libraries
- `apache-airflow-providers-snowflake` - Snowflake integration
- `apache-airflow-providers-microsoft-azure` - Azure integration
- `astronomer-cosmos` - dbt + Airflow integration
- `azure-storage-blob` - Azure Blob Storage SDK
- `dbt-snowflake` - dbt Snowflake adapter
- `requests` - HTTP API calls

---

## ğŸ”„ Pipeline Flow

### **DAG 1: Twitch Ingestion Pipeline** (`twitch_ingestion_pipeline`)
**Schedule:** Every 15 minutes  
**Tags:** `twitch`, `ingestion`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  extract_and_load_task   â”‚ â†’ Fetch from Twitch API
â”‚                          â”‚   & Upload to Azure Blob
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   refresh_snowpipes      â”‚ â†’ Trigger Snowflake ingestion
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What it does:**
1. Authenticates with Twitch OAuth
2. Fetches top 100 live streams
3. Batch retrieves user and game details
4. Uploads JSON files to Azure Blob Storage (`streams/`, `users/`, `games/`)
5. Refreshes Snowflake pipes to ingest new data

### **DAG 2: dbt Transform Pipeline** (`dbt_transform_pipeline`)
**Schedule:** Hourly  
**Tags:** `twitch`, `dbt`, `transform`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dbt_build_group        â”‚ â†’ Run all dbt models
â”‚   â”œâ”€â”€ stg_streams        â”‚   (staging views)
â”‚   â”œâ”€â”€ stg_users          â”‚
â”‚   â”œâ”€â”€ stg_games          â”‚
â”‚   â”œâ”€â”€ dim_streamers      â”‚   (dimension tables)
â”‚   â”œâ”€â”€ dim_games          â”‚
â”‚   â””â”€â”€ fact_stream_       â”‚   (fact tables)
â”‚       snapshots          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What it does:**
1. Runs dbt staging models (views in `STAGING` schema)
2. Builds dimension tables (streamers, games)
3. Creates fact table with incremental stream snapshots
4. Runs data quality tests

---

## ğŸ“ Project Structure

```
Twitch_PipeLine_dbt_dag/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dag_twitch_ingest.py          # Extraction & loading DAG
â”‚   â”œâ”€â”€ dag_dbt_transform.py          # dbt transformation DAG
â”‚   â”œâ”€â”€ twitch_extraction.py          # Twitch API logic
â”‚   â””â”€â”€ twitch_pipeline/              # dbt project
â”‚       â”œâ”€â”€ dbt_project.yml           # dbt configuration
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ staging/              # Staging views
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_streams.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_users.sql
â”‚       â”‚   â”‚   â””â”€â”€ stg_games.sql
â”‚       â”‚   â””â”€â”€ marts/                # Analytics tables
â”‚       â”‚       â”œâ”€â”€ dim_streamers.sql
â”‚       â”‚       â”œâ”€â”€ dim_games.sql
â”‚       â”‚       â””â”€â”€ fact_stream_snapshots.sql
â”‚       â”œâ”€â”€ macros/
â”‚       â”‚   â””â”€â”€ get_current_timestamp.sql
â”‚       â””â”€â”€ tests/
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ Dockerfile                         # Astronomer Runtime image
â”œâ”€â”€ airflow_settings.yaml              # Local connections & variables
â””â”€â”€ README.md                          # This file
```

---

## ğŸ“¦ Prerequisites

Before you begin, ensure you have:

- âœ… **Docker Desktop** installed and running
- âœ… **Astronomer CLI** (`astro`) installed: [Installation Guide](https://docs.astronomer.io/astro/cli/install-cli)
- âœ… **Twitch Developer Account** with API credentials ([Get here](https://dev.twitch.tv/))
- âœ… **Azure Storage Account** with Blob Storage enabled
- âœ… **Snowflake Account** with appropriate permissions
- âœ… **Git** for version control

### Required Accounts & Credentials

| Service | What You Need |
|---------|---------------|
| **Twitch** | Client ID, Client Secret |
| **Azure** | Storage Account Connection String |
| **Snowflake** | Account URL, Username, Password, Warehouse, Database |

---

## ğŸš€ Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/mohamed-k-farahat/End-to-End-Twitch-Data-Pipeline.git
cd Twitch_PipeLine_dbt_dag
```

### 2ï¸âƒ£ Install Astronomer CLI

```bash
# Windows (PowerShell)
winget install -e --id Astronomer.Astro

# macOS
brew install astro

# Linux
curl -sSL install.astronomer.io | sudo bash -s
```

### 3ï¸âƒ£ Set Up Snowflake

Run the following SQL in your Snowflake worksheet:

```sql
-- Create database and schema
CREATE DATABASE ANALYTICS;
CREATE SCHEMA ANALYTICS.PUBLIC;
CREATE SCHEMA ANALYTICS.STAGING;

-- Create raw tables
CREATE TABLE ANALYTICS.PUBLIC.RAW_STREAMS (
    RAW_JSON VARIANT,
    LOADED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

CREATE TABLE ANALYTICS.PUBLIC.RAW_USERS (
    RAW_JSON VARIANT,
    LOADED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

CREATE TABLE ANALYTICS.PUBLIC.RAW_GAMES (
    RAW_JSON VARIANT,
    LOADED_AT TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()
);

-- Create storage integration with Azure
CREATE STORAGE INTEGRATION azure_twitch_integration
  TYPE = EXTERNAL_STAGE
  STORAGE_PROVIDER = 'AZURE'
  ENABLED = TRUE
  AZURE_TENANT_ID = '<your-tenant-id>'
  STORAGE_ALLOWED_LOCATIONS = ('azure://<storage-account>.blob.core.windows.net/twitch-raw-data/');

-- Create stages
CREATE STAGE azure_streams_stage
  STORAGE_INTEGRATION = azure_twitch_integration
  URL = 'azure://<storage-account>.blob.core.windows.net/twitch-raw-data/streams/';

CREATE STAGE azure_users_stage
  STORAGE_INTEGRATION = azure_twitch_integration
  URL = 'azure://<storage-account>.blob.core.windows.net/twitch-raw-data/users/';

CREATE STAGE azure_games_stage
  STORAGE_INTEGRATION = azure_twitch_integration
  URL = 'azure://<storage-account>.blob.core.windows.net/twitch-raw-data/games/';

-- Create Snowpipes
CREATE PIPE pipe_streams
  AUTO_INGEST = TRUE
  AS
  COPY INTO RAW_STREAMS(RAW_JSON, LOADED_AT)
  FROM (
    SELECT $1, CURRENT_TIMESTAMP()
    FROM @azure_streams_stage
  )
  FILE_FORMAT = (TYPE = 'JSON');

CREATE PIPE pipe_users
  AUTO_INGEST = TRUE
  AS
  COPY INTO RAW_USERS(RAW_JSON, LOADED_AT)
  FROM (
    SELECT $1, CURRENT_TIMESTAMP()
    FROM @azure_users_stage
  )
  FILE_FORMAT = (TYPE = 'JSON');

CREATE PIPE pipe_games
  AUTO_INGEST = TRUE
  AS
  COPY INTO RAW_GAMES(RAW_JSON, LOADED_AT)
  FROM (
    SELECT $1, CURRENT_TIMESTAMP()
    FROM @azure_games_stage
  )
  FILE_FORMAT = (TYPE = 'JSON');
```

### 4ï¸âƒ£ Configure Azure Event Grid (Optional for Auto-Ingest)

For automatic Snowpipe triggering, set up Azure Event Grid to notify Snowflake when new blobs are created. Follow [Snowflake's Azure Event Grid guide](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-auto-azure).

---

## âš™ï¸ Configuration

### Edit `airflow_settings.yaml`

```yaml
airflow:
  connections:
    - conn_id: azure_blob_conn
      conn_type: generic
      extra: |
        {
          "connection_string": "DefaultEndpointsProtocol=https;AccountName=<your-account>;AccountKey=<your-key>;EndpointSuffix=core.windows.net"
        }
    
    - conn_id: snowflake_pipe_conn
      conn_type: snowflake
      login: <your-username>
      password: <your-password>
      schema: PUBLIC
      extra: |
        {
          "account": "<your-account>.snowflakecomputing.com",
          "warehouse": "COMPUTE_WH",
          "database": "ANALYTICS",
          "region": "<your-region>",
          "role": "ACCOUNTADMIN"
        }
    
    - conn_id: snowflake_dbt_conn
      conn_type: snowflake
      login: <your-username>
      password: <your-password>
      schema: PUBLIC
      extra: |
        {
          "account": "<your-account>.snowflakecomputing.com",
          "warehouse": "COMPUTE_WH",
          "database": "ANALYTICS",
          "region": "<your-region>",
          "role": "ACCOUNTADMIN"
        }

  variables:
    - variable_name: TWITCH_CLIENT_ID
      variable_value: <your-twitch-client-id>
    
    - variable_name: TWITCH_CLIENT_SECRET
      variable_value: <your-twitch-client-secret>
```

---

## ğŸƒ Running the Pipeline

### Start Airflow Locally

```bash
# Start all Airflow services
astro dev start
```

This will spin up **5 Docker containers**:
- ğŸ—„ï¸ **Postgres** - Metadata database
- ğŸ“… **Scheduler** - Task scheduling
- ğŸ”„ **Triggerer** - Deferred tasks
- ğŸ–¥ï¸ **Webserver** - UI at http://localhost:8080
- âš™ï¸ **DAG Processor** - DAG parsing

**Default Credentials:**  
Username: `admin`  
Password: `admin`

### Access the Airflow UI

1. Open your browser to **http://localhost:8080**
2. Log in with `admin` / `admin`
3. Enable the DAGs:
   - `twitch_ingestion_pipeline`
   - `dbt_transform_pipeline`

### Trigger the Pipeline Manually

```bash
# Trigger ingestion DAG
astro dev run dags trigger twitch_ingestion_pipeline

# Trigger transformation DAG
astro dev run dags trigger dbt_transform_pipeline
```

### View Logs

```bash
# Follow scheduler logs
astro dev logs -f scheduler

# Follow webserver logs
astro dev logs -f webserver
```

### Stop the Environment

```bash
astro dev stop
```

---

## ğŸ“Š Data Models

### Staging Layer (`ANALYTICS.STAGING`)

| Model | Type | Description |
|-------|------|-------------|
| `stg_streams` | View | Flattened stream data with snapshot timestamps |
| `stg_users` | View | Parsed user/streamer details |
| `stg_games` | View | Game metadata |

### Marts Layer (`ANALYTICS.PUBLIC`)

| Model | Type | Strategy | Description |
|-------|------|----------|-------------|
| `dim_streamers` | Table | - | Dimension table for streamers with SCD Type-1 |
| `dim_games` | Table | - | Dimension table for games |
| `fact_stream_snapshots` | Table | Incremental (Merge) | Fact table with stream metrics per snapshot |

### Sample Queries

```sql
-- Top 10 streamers by total viewers
SELECT 
    ds.streamer_name,
    SUM(fss.viewer_count) AS total_viewers,
    COUNT(*) AS snapshot_count
FROM ANALYTICS.PUBLIC.FACT_STREAM_SNAPSHOTS fss
JOIN ANALYTICS.PUBLIC.DIM_STREAMERS ds ON fss.streamer_id = ds.streamer_id
GROUP BY ds.streamer_name
ORDER BY total_viewers DESC
LIMIT 10;

-- Most popular games
SELECT 
    dg.game_name,
    COUNT(DISTINCT fss.stream_id) AS unique_streams,
    AVG(fss.viewer_count) AS avg_viewers
FROM ANALYTICS.PUBLIC.FACT_STREAM_SNAPSHOTS fss
JOIN ANALYTICS.PUBLIC.DIM_GAMES dg ON fss.game_id = dg.game_id
GROUP BY dg.game_name
ORDER BY unique_streams DESC;
```

---

## ğŸ“ˆ Monitoring

### Airflow UI
- **DAG Runs:** Track success/failure rates
- **Task Duration:** Identify bottlenecks
- **Logs:** Debug errors in real-time
- **Gantt Chart:** Visualize task dependencies

### dbt Docs

Generate and view dbt documentation:

```bash
# SSH into the scheduler container
astro dev bash -s scheduler

# Navigate to dbt project
cd dags/twitch_pipeline

# Generate docs
dbt docs generate --profiles-dir .

# Serve docs (if dbt docs serve is available)
dbt docs serve --profiles-dir .
```

### Snowflake Monitoring

```sql
-- Check Snowpipe status
SELECT * FROM TABLE(INFORMATION_SCHEMA.PIPE_USAGE_HISTORY(
    DATE_RANGE_START=>DATEADD('hour', -24, CURRENT_TIMESTAMP()),
    PIPE_NAME=>'ANALYTICS.PUBLIC.PIPE_STREAMS'
));

-- Check copy history
SELECT * FROM TABLE(INFORMATION_SCHEMA.COPY_HISTORY(
    TABLE_NAME=>'ANALYTICS.PUBLIC.RAW_STREAMS',
    START_TIME=>DATEADD('hour', -24, CURRENT_TIMESTAMP())
));
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### **DAG not appearing in Airflow UI**
```bash
# Check for parsing errors
astro dev run dags list

# Validate Python syntax
python dags/dag_twitch_ingest.py
```

#### **Snowflake connection failed**
- Verify account identifier format: `<account>.<region>.snowflakecomputing.com`
- Check username, password, and role permissions
- Ensure warehouse is running

#### **Azure Blob upload failed**
- Validate connection string in `airflow_settings.yaml`
- Check container name: `twitch-raw-data`
- Verify Azure Storage access keys

#### **dbt models failing**
```bash
# Test connection
astro dev bash -s scheduler
cd dags/twitch_pipeline
dbt debug --profiles-dir .

# Run specific model
dbt run --select stg_streams --profiles-dir .
```

#### **Port already in use**
```bash
# Change Airflow port in docker-compose.override.yml
astro config set webserver.port 8081
```

### Logs Location

```bash
# View all logs
astro dev logs

# Scheduler logs only
astro dev logs -f scheduler

# Follow logs for specific task
# (In Airflow UI: DAG > Task > Log)
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Mohamed K. Farahat**

- GitHub: [@mohamed-k-farahat](https://github.com/mohamed-k-farahat)
- Repository: [End-to-End-Twitch-Data-Pipeline](https://github.com/mohamed-k-farahat/End-to-End-Twitch-Data-Pipeline)

---

## ğŸ™ Acknowledgments

- [Astronomer](https://www.astronomer.io/) for the amazing Airflow platform
- [dbt Labs](https://www.getdbt.com/) for the dbt framework
- [Twitch](https://dev.twitch.tv/) for providing the API
- [Snowflake](https://www.snowflake.com/) for the data warehouse
- [Microsoft Azure](https://azure.microsoft.com/) for cloud storage

---

<div align="center">

**â­ Star this repo if you find it helpful!**

Made with â¤ï¸ and â˜• by Mohamed Farahat

</div>
